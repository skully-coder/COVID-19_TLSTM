{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\r\n",
    "import pickle\r\n",
    "import numpy as np\r\n",
    "import math\r\n",
    "import tensorflow.compat.v1 as tf\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from LSTMmodel import LSTM"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abhinav\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def load_pkl(path):\r\n",
    "    with open(path,'rb') as f:\r\n",
    "        obj = pickle.load(f)\r\n",
    "        return obj"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def training(path, fold, training_epochs, train_dropout_prob, hidden_dim, fc_dim, key, model_path, learning_rate=[1e-5, 2e-2], lr_decay=2000,):\r\n",
    "    # train data\r\n",
    "    path_string = path + '/TrainData.seqs'\r\n",
    "    data_train_batches = load_pkl(path_string)\r\n",
    "\r\n",
    "    path_string = path + '/TrainLabel.seqs'\r\n",
    "    labels_train_batches = load_pkl(path_string)\r\n",
    "\r\n",
    "    number_train_batches = len(data_train_batches)\r\n",
    "\r\n",
    "    input_dim = np.array(data_train_batches[0]).shape[2]\r\n",
    "    output_dim = np.array(labels_train_batches[0]).shape[1]\r\n",
    "\r\n",
    "    print(\"Train data is loaded!\")\r\n",
    "\r\n",
    "    path_string = path + '/TestData.seqs'\r\n",
    "    data_test_batches = load_pkl(path_string)\r\n",
    "\r\n",
    "    path_string = path + '/TestLabel.seqs'\r\n",
    "    labels_test_batches = load_pkl(path_string)\r\n",
    "\r\n",
    "    number_test_batches = len(data_test_batches)\r\n",
    "\r\n",
    "    print(\"Test data is loaded!\")\r\n",
    "\r\n",
    "    # model built\r\n",
    "    lstm = LSTM(input_dim, output_dim, hidden_dim, fc_dim, key)\r\n",
    "    cross_entropy, y_pred, y, logits, labels = lstm.get_cost_acc()\r\n",
    "    lr = learning_rate[0] + tf.train.exponential_decay(learning_rate[1], lstm.step, lr_decay, 1 / np.e)\r\n",
    "    optimizer = tf.train.AdamOptimizer(\r\n",
    "        learning_rate=lr).minimize(cross_entropy)\r\n",
    "    init = tf.global_variables_initializer()\r\n",
    "    saver = tf.train.Saver()\r\n",
    "    best_valid_loss = 1e10\r\n",
    "\r\n",
    "    # train\r\n",
    "    with tf.Session() as sess:\r\n",
    "        sess.run(init)\r\n",
    "        valid_acc = []\r\n",
    "        valid_auc = []\r\n",
    "        valid_loss = []\r\n",
    "        for epoch in range(training_epochs):\r\n",
    "            # Loop over all batches\r\n",
    "            for i in range(number_train_batches):\r\n",
    "                # batch_xs is [number of patients x sequence length x input dimensionality]\r\n",
    "                batch_xs, batch_ys = data_train_batches[i], labels_train_batches[i]\r\n",
    "                step = epoch * number_train_batches + i\r\n",
    "                sess.run(optimizer, feed_dict={\r\n",
    "                         lstm.input: batch_xs, lstm.labels: batch_ys, lstm.keep_prob: train_dropout_prob, lstm.step: step})\r\n",
    "                print('Training epoch ' + str(epoch) +\r\n",
    "                      ' batch ' + str(i) + ' done')\r\n",
    "\r\n",
    "            # valid\r\n",
    "            loss = []\r\n",
    "            Y_pred = []\r\n",
    "            Y_true = []\r\n",
    "            Labels = []\r\n",
    "            Logits = []\r\n",
    "            for k in range(number_train_batches):  #\r\n",
    "                if (len(data_train_batches[k]) < fold):\r\n",
    "                    continue\r\n",
    "                batch_xs, batch_ys = [data_train_batches[k]\r\n",
    "                                      [-fold]], [labels_train_batches[k][-fold]]\r\n",
    "                c_train, y_pred_train, y_train, logits_train, labels_train = sess.run(lstm.get_cost_acc(),\r\n",
    "                                                                                      feed_dict={\r\n",
    "                                                                                          lstm.input: batch_xs,\r\n",
    "                                                                                          lstm.labels: batch_ys,\r\n",
    "                                                                                          lstm.keep_prob: train_dropout_prob})\r\n",
    "                loss.append(c_train)\r\n",
    "                if k > 0:\r\n",
    "                    Y_true = np.concatenate([Y_true, y_train], 0)\r\n",
    "                    Y_pred = np.concatenate([Y_pred, y_pred_train], 0)\r\n",
    "                    Labels = np.concatenate([Labels, labels_train], 0)\r\n",
    "                    Logits = np.concatenate([Logits, logits_train], 0)\r\n",
    "                else:\r\n",
    "                    Y_true = y_train\r\n",
    "                    Y_pred = y_pred_train\r\n",
    "                    Labels = labels_train\r\n",
    "                    Logits = logits_train\r\n",
    "\r\n",
    "            total_acc = accuracy_score(Y_true, Y_pred)\r\n",
    "            total_auc = roc_auc_score(Labels, Logits, average='micro')\r\n",
    "            total_auc_macro = roc_auc_score(Labels, Logits, average='macro')\r\n",
    "            mean_loss = np.mean(loss)\r\n",
    "            print(\"validation Accuracy = {:.3f}\".format(total_acc))\r\n",
    "            print(\"validation AUC = {:.3f}\".format(total_auc))\r\n",
    "            print(\"validation AUC Macro = {:.3f}\".format(total_auc_macro))\r\n",
    "            print(\"validation Loss = {:.3f}\".format(mean_loss))\r\n",
    "            print('epoch ' + str(epoch) + ' done........................')\r\n",
    "\r\n",
    "            valid_acc.append(total_acc)\r\n",
    "            valid_auc.append(total_auc)\r\n",
    "            valid_loss.append(mean_loss)\r\n",
    "\r\n",
    "            if(mean_loss <= best_valid_loss):\r\n",
    "                print(\"[*] Best loss so far! \")\r\n",
    "                saver.save(sess, model_path+'model'+str(fold)+'/')\r\n",
    "                print(\"[*] Model saved at\", model_path +\r\n",
    "                      'model'+str(fold)+'/', flush=True)\r\n",
    "\r\n",
    "        pickle.dump(valid_acc, open(\r\n",
    "            'results/valid_acc'+str(fold)+'.seqs', 'wb'), -1)\r\n",
    "        pickle.dump(valid_auc, open(\r\n",
    "            'results/valid_auc'+str(fold)+'.seqs', 'wb'), -1)\r\n",
    "        pickle.dump(valid_loss, open(\r\n",
    "            'results/valid_loss' + str(fold) + '.seqs', 'wb'), -1)\r\n",
    "\r\n",
    "        print(\"Fold \"+str(fold)+\" training is over!\")\r\n",
    "        saver.save(sess, model_path+'model'+str(fold)+'/')\r\n",
    "        print(\"[******] Model saved at\", model_path +\r\n",
    "              'model'+str(fold)+'/', flush=True)\r\n",
    "\r\n",
    "        # test\r\n",
    "        loss = []\r\n",
    "        Y_pred = []\r\n",
    "        Y_true = []\r\n",
    "        Labels = []\r\n",
    "        Logits = []\r\n",
    "        for i in range(number_test_batches):  #\r\n",
    "            batch_xs, batch_ys = data_test_batches[i], labels_test_batches[i]\r\n",
    "            c_train, y_pred_train, y_train, logits_train, labels_train = sess.run(lstm.get_cost_acc(),\r\n",
    "                                                                                  feed_dict={lstm.input: batch_xs,\r\n",
    "                                                                                             lstm.labels: batch_ys,\r\n",
    "                                                                                             lstm.keep_prob: train_dropout_prob})\r\n",
    "            loss.append(c_train)\r\n",
    "            if i > 0:\r\n",
    "                Y_true = np.concatenate([Y_true, y_train], 0)\r\n",
    "                Y_pred = np.concatenate([Y_pred, y_pred_train], 0)\r\n",
    "                Labels = np.concatenate([Labels, labels_train], 0)\r\n",
    "                Logits = np.concatenate([Logits, logits_train], 0)\r\n",
    "            else:\r\n",
    "                Y_true = y_train\r\n",
    "                Y_pred = y_pred_train\r\n",
    "                Labels = labels_train\r\n",
    "                Logits = logits_train\r\n",
    "        total_acc = accuracy_score(Y_true, Y_pred)\r\n",
    "        total_auc = roc_auc_score(Labels, Logits, average='micro')\r\n",
    "        total_auc_macro = roc_auc_score(Labels, Logits, average='macro')\r\n",
    "        print(\"Test Accuracy = {:.3f}\".format(total_acc))\r\n",
    "        print(\"Test AUC = {:.3f}\".format(total_auc))\r\n",
    "        print(\"Test AUC Macro = {:.3f}\".format(total_auc_macro))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def testing(path, hidden_dim, fc_dim, key, model_path):\r\n",
    "    path_string = path + '/TestData.seqs'\r\n",
    "    data_test_batches = load_pkl(path_string)\r\n",
    "\r\n",
    "    path_string = path + '/TestLabel.seqs'\r\n",
    "    labels_test_batches = load_pkl(path_string)\r\n",
    "\r\n",
    "    number_test_batches = len(data_test_batches)\r\n",
    "\r\n",
    "    print(\"Test data is loaded!\")\r\n",
    "\r\n",
    "    input_dim = np.array(data_test_batches[0]).shape[2]\r\n",
    "    output_dim = np.array(labels_test_batches[0]).shape[1]\r\n",
    "\r\n",
    "    test_dropout_prob = 1.0\r\n",
    "    lstm_load = LSTM(input_dim, output_dim, hidden_dim, fc_dim, key)\r\n",
    "\r\n",
    "    saver = tf.train.Saver()\r\n",
    "    with tf.Session() as sess:\r\n",
    "        saver.restore(sess, model_path)\r\n",
    "\r\n",
    "        Y_true = []\r\n",
    "        Y_pred = []\r\n",
    "        Logits = []\r\n",
    "        Labels = []\r\n",
    "        for i in range(number_test_batches):\r\n",
    "            batch_xs, batch_ys = data_test_batches[i], labels_test_batches[i]\r\n",
    "            c_test, y_pred_test, y_test, logits_test, labels_test = sess.run(lstm_load.get_cost_acc(),\r\n",
    "                                                                             feed_dict={lstm_load.input: batch_xs,\r\n",
    "                                                                                        lstm_load.labels: batch_ys,\r\n",
    "                                                                                        lstm_load.keep_prob: test_dropout_prob})\r\n",
    "            if i > 0:\r\n",
    "                Y_true = np.concatenate([Y_true, y_test], 0)\r\n",
    "                Y_pred = np.concatenate([Y_pred, y_pred_test], 0)\r\n",
    "                Labels = np.concatenate([Labels, labels_test], 0)\r\n",
    "                Logits = np.concatenate([Logits, logits_test], 0)\r\n",
    "            else:\r\n",
    "                Y_true = y_test\r\n",
    "                Y_pred = y_pred_test\r\n",
    "                Labels = labels_test\r\n",
    "                Logits = logits_test\r\n",
    "\r\n",
    "        total_auc = roc_auc_score(Labels, Logits, average='micro')\r\n",
    "        total_auc_macro = roc_auc_score(Labels, Logits, average='macro')\r\n",
    "        total_acc = accuracy_score(Y_true, Y_pred)\r\n",
    "        print(\"Test Accuracy = {:.3f}\".format(total_acc))\r\n",
    "        print(\"Test AUC Micro = {:.3f}\".format(total_auc))\r\n",
    "        print(\"Test AUC Macro = {:.3f}\".format(total_auc_macro))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def testing_Uncertainty(path, test_dropout_prob, hidden_dim, fc_dim, key, model_path, model_num):\r\n",
    "\r\n",
    "    path_string = path + '/batches_data_test.seqs'\r\n",
    "    data_test_batches = load_pkl(path_string)\r\n",
    "\r\n",
    "    path_string = path + '/batches_label_test.seqs'\r\n",
    "    labels_test_batches = load_pkl(path_string)\r\n",
    "\r\n",
    "    print(\"Test data is loaded!\")\r\n",
    "\r\n",
    "    input_dim = np.array(data_test_batches[0]).shape[2]\r\n",
    "    output_dim = np.array(labels_test_batches[0]).shape[1]\r\n",
    "\r\n",
    "    test_dropout_prob = test_dropout_prob\r\n",
    "\r\n",
    "    lstm_load = LSTM(input_dim, output_dim, hidden_dim, fc_dim, key)\r\n",
    "    saver = tf.train.Saver()\r\n",
    "\r\n",
    "    with tf.Session() as sess:\r\n",
    "        saver.restore(sess, model_path)\r\n",
    "\r\n",
    "        acc_in_time_length = []\r\n",
    "        auc_in_time_length = []\r\n",
    "        uncertainty_in_time_length = []\r\n",
    "        batch_xs, batch_ys = data_test_batches[0], labels_test_batches[0]\r\n",
    "        time_length = len(batch_xs[0])\r\n",
    "\r\n",
    "        for length in range(time_length-12, time_length):\r\n",
    "            # 时间截断\r\n",
    "            batch_xs_sub = np.array(batch_xs)[:, :length].tolist()\r\n",
    "\r\n",
    "            ACCs = []\r\n",
    "            AUCs = []\r\n",
    "            Pcs = []\r\n",
    "            for j in range(model_num):\r\n",
    "\r\n",
    "                c_test, y_pred_test, y_test, logits_test, labels_test = sess.run(lstm_load.get_cost_acc(),\r\n",
    "                                                                                 feed_dict={lstm_load.input: batch_xs_sub,\r\n",
    "                                                                                            lstm_load.labels: batch_ys,\r\n",
    "                                                                                            lstm_load.keep_prob: test_dropout_prob})\r\n",
    "                Y_true = y_test\r\n",
    "                Y_pred = y_pred_test\r\n",
    "                Labels = labels_test\r\n",
    "                Logits = logits_test\r\n",
    "\r\n",
    "                total_auc_macro = roc_auc_score(\r\n",
    "                    Labels, Logits, average='macro')\r\n",
    "                total_acc = accuracy_score(Y_true, Y_pred)\r\n",
    "                print(\"Test Accuracy = {:.3f}\".format(total_acc))\r\n",
    "                print(\"Test AUC Micro = {:.3f}\".format(total_auc_macro))\r\n",
    "                print(\"Test AUC Macro = {:.3f}\".format(total_auc_macro))\r\n",
    "                ACCs.append(total_acc)\r\n",
    "                AUCs.append(total_auc_macro)\r\n",
    "\r\n",
    "                C = np.bincount(Y_pred)\r\n",
    "                Pc = [x/np.sum(C) for x in C]\r\n",
    "                Pcs.append(Pc)\r\n",
    "\r\n",
    "            meanACC = np.mean(ACCs)\r\n",
    "            meanAUC = np.mean(AUCs)\r\n",
    "\r\n",
    "            # total uncertainty\r\n",
    "            p_avg = np.array(Pcs).mean(axis=0)\r\n",
    "            total_uncertainty = sum((-x)*math.log(x, 2) for x in p_avg)\r\n",
    "            # expected data uncertainty\r\n",
    "            entropy = [sum((-x) * math.log(x, 2) for x in i) for i in Pcs]\r\n",
    "            expected_data_uncertainty = np.array(entropy).mean(axis=0)\r\n",
    "            # model uncertainty\r\n",
    "            model_uncertainty = total_uncertainty-expected_data_uncertainty\r\n",
    "            print('mean ACC: ' + str(meanACC)+' mean AUC: ' +\r\n",
    "                  str(meanAUC)+' uncertainty: ' + str(model_uncertainty))\r\n",
    "\r\n",
    "            acc_in_time_length.append(meanACC)\r\n",
    "            auc_in_time_length.append(meanAUC)\r\n",
    "            uncertainty_in_time_length.append(model_uncertainty)\r\n",
    "\r\n",
    "    return acc_in_time_length, auc_in_time_length, uncertainty_in_time_length"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def main(training_mode, fold, data_path, learning_rate, lr_decay, training_epochs, dropout_prob, hidden_dim, fc_dim, model_path, model_num=0):\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    :param training_mode:  1train，0test，2uncertainty\r\n",
    "    :param fold: 5-fold\r\n",
    "    :param data_path:\r\n",
    "    :param learning_rate:\r\n",
    "    :param training_epochs:\r\n",
    "    :param dropout_prob: dropout，1 keep all\r\n",
    "    :param hidden_dim:\r\n",
    "    :param fc_dim:\r\n",
    "    :param model_path: save/load model path\r\n",
    "    :param model_num: number of models when uncertainty\r\n",
    "    \"\"\"\r\n",
    "    training_mode = int(training_mode)\r\n",
    "    path = str(data_path)\r\n",
    "\r\n",
    "    # train\r\n",
    "    if training_mode == 1:\r\n",
    "        learning_rate = learning_rate\r\n",
    "        lr_decay = lr_decay\r\n",
    "        training_epochs = int(training_epochs)\r\n",
    "        dropout_prob = float(dropout_prob)\r\n",
    "        hidden_dim = int(hidden_dim)\r\n",
    "        fc_dim = int(fc_dim)\r\n",
    "        model_path = str(model_path)\r\n",
    "        training(path, fold, training_epochs, dropout_prob, hidden_dim,\r\n",
    "                 fc_dim, training_mode, model_path, learning_rate, lr_decay)\r\n",
    "\r\n",
    "    # test\r\n",
    "    elif training_mode == 0:\r\n",
    "        hidden_dim = int(hidden_dim)\r\n",
    "        fc_dim = int(fc_dim)\r\n",
    "        model_path = str(model_path)\r\n",
    "        testing(path, hidden_dim, fc_dim, training_mode, model_path)\r\n",
    "\r\n",
    "    # test with mc_dropout\r\n",
    "    elif training_mode == 2:\r\n",
    "        dropout_prob = float(dropout_prob)\r\n",
    "        hidden_dim = int(hidden_dim)\r\n",
    "        fc_dim = int(fc_dim)\r\n",
    "        model_path = str(model_path)\r\n",
    "        model_num = model_num\r\n",
    "        acc_in_time_length, auc_in_time_length, uncertainty_in_time_length = testing_Uncertainty(\r\n",
    "            path, dropout_prob, hidden_dim, fc_dim, training_mode, model_path, model_num)\r\n",
    "        print(acc_in_time_length)\r\n",
    "        print(auc_in_time_length)\r\n",
    "        print(uncertainty_in_time_length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "\r\n",
    "    main(training_mode=1, fold=1, data_path='../../BatchData', learning_rate=[\r\n",
    "         1e-5, 2e-2], lr_decay=2000, training_epochs=15, dropout_prob=0.25, hidden_dim=64, fc_dim=32, model_path='../../model/', model_num=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train data is loaded!\n",
      "Test data is loaded!\n",
      "WARNING:tensorflow:From C:\\Users\\Abhinav\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Abhinav\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Training epoch 0 batch 0 done\n",
      "Training epoch 0 batch 1 done\n",
      "Training epoch 0 batch 2 done\n",
      "Training epoch 0 batch 3 done\n",
      "Training epoch 0 batch 4 done\n",
      "Training epoch 0 batch 5 done\n",
      "Training epoch 0 batch 6 done\n",
      "Training epoch 0 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.719\n",
      "validation AUC Macro = 0.417\n",
      "validation Loss = 0.897\n",
      "epoch 0 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 1 batch 0 done\n",
      "Training epoch 1 batch 1 done\n",
      "Training epoch 1 batch 2 done\n",
      "Training epoch 1 batch 3 done\n",
      "Training epoch 1 batch 4 done\n",
      "Training epoch 1 batch 5 done\n",
      "Training epoch 1 batch 6 done\n",
      "Training epoch 1 batch 7 done\n",
      "validation Accuracy = 0.625\n",
      "validation AUC = 0.672\n",
      "validation AUC Macro = 0.583\n",
      "validation Loss = 0.574\n",
      "epoch 1 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 2 batch 0 done\n",
      "Training epoch 2 batch 1 done\n",
      "Training epoch 2 batch 2 done\n",
      "Training epoch 2 batch 3 done\n",
      "Training epoch 2 batch 4 done\n",
      "Training epoch 2 batch 5 done\n",
      "Training epoch 2 batch 6 done\n",
      "Training epoch 2 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.797\n",
      "validation AUC Macro = 0.625\n",
      "validation Loss = 0.620\n",
      "epoch 2 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 3 batch 0 done\n",
      "Training epoch 3 batch 1 done\n",
      "Training epoch 3 batch 2 done\n",
      "Training epoch 3 batch 3 done\n",
      "Training epoch 3 batch 4 done\n",
      "Training epoch 3 batch 5 done\n",
      "Training epoch 3 batch 6 done\n",
      "Training epoch 3 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.625\n",
      "validation AUC Macro = 0.167\n",
      "validation Loss = 0.700\n",
      "epoch 3 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 4 batch 0 done\n",
      "Training epoch 4 batch 1 done\n",
      "Training epoch 4 batch 2 done\n",
      "Training epoch 4 batch 3 done\n",
      "Training epoch 4 batch 4 done\n",
      "Training epoch 4 batch 5 done\n",
      "Training epoch 4 batch 6 done\n",
      "Training epoch 4 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.672\n",
      "validation AUC Macro = 0.500\n",
      "validation Loss = 0.628\n",
      "epoch 4 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 5 batch 0 done\n",
      "Training epoch 5 batch 1 done\n",
      "Training epoch 5 batch 2 done\n",
      "Training epoch 5 batch 3 done\n",
      "Training epoch 5 batch 4 done\n",
      "Training epoch 5 batch 5 done\n",
      "Training epoch 5 batch 6 done\n",
      "Training epoch 5 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.781\n",
      "validation AUC Macro = 0.583\n",
      "validation Loss = 0.535\n",
      "epoch 5 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 6 batch 0 done\n",
      "Training epoch 6 batch 1 done\n",
      "Training epoch 6 batch 2 done\n",
      "Training epoch 6 batch 3 done\n",
      "Training epoch 6 batch 4 done\n",
      "Training epoch 6 batch 5 done\n",
      "Training epoch 6 batch 6 done\n",
      "Training epoch 6 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.781\n",
      "validation AUC Macro = 0.583\n",
      "validation Loss = 0.577\n",
      "epoch 6 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 7 batch 0 done\n",
      "Training epoch 7 batch 1 done\n",
      "Training epoch 7 batch 2 done\n",
      "Training epoch 7 batch 3 done\n",
      "Training epoch 7 batch 4 done\n",
      "Training epoch 7 batch 5 done\n",
      "Training epoch 7 batch 6 done\n",
      "Training epoch 7 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.875\n",
      "validation AUC Macro = 0.833\n",
      "validation Loss = 0.481\n",
      "epoch 7 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 8 batch 0 done\n",
      "Training epoch 8 batch 1 done\n",
      "Training epoch 8 batch 2 done\n",
      "Training epoch 8 batch 3 done\n",
      "Training epoch 8 batch 4 done\n",
      "Training epoch 8 batch 5 done\n",
      "Training epoch 8 batch 6 done\n",
      "Training epoch 8 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.750\n",
      "validation AUC Macro = 0.500\n",
      "validation Loss = 0.635\n",
      "epoch 8 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 9 batch 0 done\n",
      "Training epoch 9 batch 1 done\n",
      "Training epoch 9 batch 2 done\n",
      "Training epoch 9 batch 3 done\n",
      "Training epoch 9 batch 4 done\n",
      "Training epoch 9 batch 5 done\n",
      "Training epoch 9 batch 6 done\n",
      "Training epoch 9 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.797\n",
      "validation AUC Macro = 0.625\n",
      "validation Loss = 0.452\n",
      "epoch 9 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 10 batch 0 done\n",
      "Training epoch 10 batch 1 done\n",
      "Training epoch 10 batch 2 done\n",
      "Training epoch 10 batch 3 done\n",
      "Training epoch 10 batch 4 done\n",
      "Training epoch 10 batch 5 done\n",
      "Training epoch 10 batch 6 done\n",
      "Training epoch 10 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.750\n",
      "validation AUC Macro = 0.500\n",
      "validation Loss = 0.501\n",
      "epoch 10 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 11 batch 0 done\n",
      "Training epoch 11 batch 1 done\n",
      "Training epoch 11 batch 2 done\n",
      "Training epoch 11 batch 3 done\n",
      "Training epoch 11 batch 4 done\n",
      "Training epoch 11 batch 5 done\n",
      "Training epoch 11 batch 6 done\n",
      "Training epoch 11 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.844\n",
      "validation AUC Macro = 0.750\n",
      "validation Loss = 0.429\n",
      "epoch 11 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 12 batch 0 done\n",
      "Training epoch 12 batch 1 done\n",
      "Training epoch 12 batch 2 done\n",
      "Training epoch 12 batch 3 done\n",
      "Training epoch 12 batch 4 done\n",
      "Training epoch 12 batch 5 done\n",
      "Training epoch 12 batch 6 done\n",
      "Training epoch 12 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.812\n",
      "validation AUC Macro = 0.667\n",
      "validation Loss = 0.493\n",
      "epoch 12 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 13 batch 0 done\n",
      "Training epoch 13 batch 1 done\n",
      "Training epoch 13 batch 2 done\n",
      "Training epoch 13 batch 3 done\n",
      "Training epoch 13 batch 4 done\n",
      "Training epoch 13 batch 5 done\n",
      "Training epoch 13 batch 6 done\n",
      "Training epoch 13 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.875\n",
      "validation AUC Macro = 0.833\n",
      "validation Loss = 0.433\n",
      "epoch 13 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n",
      "Training epoch 14 batch 0 done\n",
      "Training epoch 14 batch 1 done\n",
      "Training epoch 14 batch 2 done\n",
      "Training epoch 14 batch 3 done\n",
      "Training epoch 14 batch 4 done\n",
      "Training epoch 14 batch 5 done\n",
      "Training epoch 14 batch 6 done\n",
      "Training epoch 14 batch 7 done\n",
      "validation Accuracy = 0.750\n",
      "validation AUC = 0.906\n",
      "validation AUC Macro = 0.917\n",
      "validation Loss = 0.297\n",
      "epoch 14 done........................\n",
      "[*] Best loss so far! \n",
      "[*] Model saved at ../../model/model1/\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/valid_acc1.seqs'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1114a552d2c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     main(training_mode=1, fold=1, data_path='../../BatchData', learning_rate=[\n\u001b[0m\u001b[0;32m      4\u001b[0m          1e-5, 2e-2], lr_decay=2000, training_epochs=15, dropout_prob=0.25, hidden_dim=64, fc_dim=32, model_path='../../model/', model_num=5)\n",
      "\u001b[1;32m<ipython-input-6-15c4327b0db8>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(training_mode, fold, data_path, learning_rate, lr_decay, training_epochs, dropout_prob, hidden_dim, fc_dim, model_path, model_num)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mfc_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfc_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         training(path, fold, training_epochs, dropout_prob, hidden_dim,\n\u001b[0m\u001b[0;32m     28\u001b[0m                  fc_dim, training_mode, model_path, learning_rate, lr_decay)\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-3340d4ae37fa>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(path, fold, training_epochs, train_dropout_prob, hidden_dim, fc_dim, key, model_path, learning_rate, lr_decay)\u001b[0m\n\u001b[0;32m     99\u001b[0m                       'model'+str(fold)+'/', flush=True)\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         pickle.dump(valid_acc, open(\n\u001b[0m\u001b[0;32m    102\u001b[0m             'results/valid_acc'+str(fold)+'.seqs', 'wb'), -1)\n\u001b[0;32m    103\u001b[0m         pickle.dump(valid_auc, open(\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/valid_acc1.seqs'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "4946931a8918bbb5130182eb31d4a5c7824764c406ba2f92c5beaee952cca07f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}